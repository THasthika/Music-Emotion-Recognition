{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: wandb in /opt/anaconda3/envs/research/lib/python3.7/site-packages (0.10.23)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: Click>=7.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: pathtools in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (3.1.14)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from wandb) (3.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/research/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "!conda install -c pytorch torchaudio\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V9yeOc53zj_0"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as audioF\n",
    "import torchaudio.transforms as audioT\n",
    "\n",
    "from torchaudio.sox_effects import apply_effects_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LqABgbwqzlyV"
   },
   "outputs": [],
   "source": [
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lL_bgQoPzqKU",
    "outputId": "c338c65a-5bb5-42a8-9f8b-71723f8b6a4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaverickuor\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t0H_aHuizscA"
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=20,\n",
    "    classes=4,\n",
    "    kernels=[2205, 256],\n",
    "    batch_size=64,\n",
    "    learning_rate=0.005,\n",
    "    dataset=\"MER_Taffc\",\n",
    "    architecture=\"1D Convolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RvGGqQ12zuJw"
   },
   "outputs": [],
   "source": [
    "# data paths\n",
    "DATA_PATH = \"../../datasets/MER_taffc/wav/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t-60JZ9nzxGe"
   },
   "outputs": [],
   "source": [
    "class MERTaffcDataSet(Dataset):\n",
    "    \"\"\"MER_Taffc dataset.\"\"\"\n",
    "    \n",
    "    folders = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "    labels = {\n",
    "        \"Q1\": torch.tensor(0),\n",
    "        \"Q2\": torch.tensor(1),\n",
    "        \"Q3\": torch.tensor(2),\n",
    "        \"Q4\": torch.tensor(3)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, root_dir, sample_rate=22050, duration=30):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.frame_count = self.sample_rate * self.duration\n",
    "        \n",
    "        all_files = []\n",
    "        for f in self.folders:\n",
    "            files = os.listdir(path.join(root_dir, f))\n",
    "            files = list(map(lambda x: (path.join(root_dir, f, x), f), files))\n",
    "            all_files.extend(files)\n",
    "            \n",
    "        self.files = all_files\n",
    "        self.count = len(all_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file, label = self.files[idx]\n",
    "        \n",
    "        x, sr = torchaudio.load(audio_file)\n",
    "        out = torch.zeros(1, self.frame_count)\n",
    "        \n",
    "        effects = [\n",
    "          [\"rate\", f\"{self.sample_rate}\"]\n",
    "        ]\n",
    "        \n",
    "        x, sr2 = torchaudio.sox_effects.apply_effects_tensor(x, sr, effects)\n",
    "        \n",
    "        if self.frame_count >= x.shape[1]:\n",
    "            out[:, :x.shape[1]] = x\n",
    "        else:\n",
    "            out[:, :] = x[:, :self.frame_count]\n",
    "        \n",
    "        \n",
    "        return (out, self.labels[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mYh_X-bIz4Kq"
   },
   "outputs": [],
   "source": [
    "class BaseAudioNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BaseAudioNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=48, kernel_size=80, stride=4),\n",
    "            nn.BatchNorm1d(num_features=48),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "\n",
    "            nn.Conv1d(in_channels=48, out_channels=48, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(num_features=48),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=48, out_channels=48, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(num_features=48),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "\n",
    "            nn.Conv1d(in_channels=48, out_channels=96, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(num_features=96),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=96, out_channels=96, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(num_features=96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "\n",
    "            nn.Conv1d(in_channels=96, out_channels=192, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=192, out_channels=192, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "\n",
    "            nn.Conv1d(in_channels=192, out_channels=384, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(num_features=384),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=384, out_channels=384, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm1d(num_features=384),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(50)\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(in_features=384*50, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.predictor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6ZkIY79W0YoC"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    dataset = MERTaffcDataSet(DATA_PATH)\n",
    "    train_len = int(len(dataset) * 0.6)\n",
    "    valid_len = int(len(dataset) * 0.2)\n",
    "    test_len = len(dataset) - (train_len + valid_len)\n",
    "    return random_split(dataset, [train_len, valid_len, test_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6MXVIVxI0aP4"
   },
   "outputs": [],
   "source": [
    "# def train(model, loader, criterion, optimizer, epochs=20):\n",
    "#     total_batches = len(loader) * epochs\n",
    "#     example_ct = 0  # number of examples seen\n",
    "#     batch_ct = 0\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         for (X, labels) in loader:\n",
    "#             loss = train_batch(X, labels, model, criterion, optimizer)\n",
    "#             example_ct +=  len(X)\n",
    "#             batch_ct += 1\n",
    "#             # Report metrics every 25th batch\n",
    "#             if ((batch_ct + 1) % 25) == 0:\n",
    "#                 train_log(loss, example_ct, epoch)\n",
    "\n",
    "# def train_batch(X, labels, model, criterion, optimizer):\n",
    "#     X = X.to(device)\n",
    "#     labels = labels.to(device)\n",
    "    \n",
    "#     # Forward pass ➡\n",
    "#     outputs = F.softmax(model(X), dim=1)\n",
    "    \n",
    "#     loss = criterion(outputs, labels)\n",
    "\n",
    "#     # Backward pass ⬅\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Step with optimizer\n",
    "#     optimizer.step()\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rOUKnsoW0b0G"
   },
   "outputs": [],
   "source": [
    "# def run():\n",
    "    \n",
    "#     (train_dataset, val_dataset, test_dataset) = get_data()\n",
    "#     train_loader = make_loader(train_dataset, 64)\n",
    "    \n",
    "#     model = BaseAudioNet()\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     train(model, train_loader, criterion, optimizer, epochs=1)\n",
    "\n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-OwqGqqQ0dMK"
   },
   "outputs": [],
   "source": [
    "# (X, label) = next(iter(dataloader))\n",
    "# X = X.to(device)\n",
    "# logits = net(X)\n",
    "# pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# y_pred = pred_probab.argmax(1)\n",
    "# print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HbqBQa0y0eyD"
   },
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(params=net.parameters())\n",
    "# loss_fn = torch.\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     x, targets = batch\n",
    "#     preds = net(x)\n",
    "#     print(preds.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "j4Pfdu7g0gkh"
   },
   "outputs": [],
   "source": [
    "# (train, x, y) = get_data()\n",
    "# t = make_loader(train, 64)\n",
    "# a = next(iter(t))\n",
    "# print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e6JOqeG70iOP"
   },
   "outputs": [],
   "source": [
    "def make_model(config):\n",
    "    net = BaseAudioNet()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VycEMsTR0joo"
   },
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    train, valid, test = get_data()\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
    "    valid_loader = make_loader(valid, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
    "\n",
    "    # Make the model\n",
    "    model = make_model(config).to(device)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, train_loader, valid_loader, test_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yKENj4ZH0lTy"
   },
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_ImarrIT0l-7"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for X, labels in test_loader:\n",
    "            X, labels = X.to(device), labels.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f\"Accuracy of the model on the {total} \" + f\"test images: {100 * correct / total}%\")\n",
    "        wandb.log({\"test_accuracy\": correct / total})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, images, \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GlWV7uod0oKi"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, _val_loader, criterion, optimizer, config):\n",
    "    # tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    \n",
    "    print(config)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for (X, labels) in loader:\n",
    "            loss = train_batch(X, labels, model, optimizer, criterion)\n",
    "            example_ct +=  len(X)\n",
    "            batch_ct += 1\n",
    "\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                train_log(loss, example_ct, epoch)\n",
    "\n",
    "\n",
    "def train_batch(X, labels, model, optimizer, criterion):\n",
    "    X = X.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass ➡\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hX2tR59n0qOr"
   },
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project='baseline-audio-only', entity='maverickuor', config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, valid_loader, test_loader, criterion, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, train_loader, valid_loader, criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "GNX2Hz4X0ruJ",
    "outputId": "e77393cc-8eae-4567-e4a5-628192cb1a6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">charmed-field-19</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/maverickuor/baseline-audio-only\" target=\"_blank\">https://wandb.ai/maverickuor/baseline-audio-only</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/maverickuor/baseline-audio-only/runs/2zyk3xe4\" target=\"_blank\">https://wandb.ai/maverickuor/baseline-audio-only/runs/2zyk3xe4</a><br/>\n",
       "                Run data is saved locally in <code>/Users/tharindu/Projects/research/notebooks/mer_taffc/wandb/run-20210324_071650-2zyk3xe4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseAudioNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv1d(1, 48, kernel_size=(80,), stride=(4,))\n",
      "    (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(48, 48, kernel_size=(3,), stride=(1,))\n",
      "    (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Conv1d(48, 48, kernel_size=(3,), stride=(1,))\n",
      "    (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv1d(48, 96, kernel_size=(3,), stride=(1,))\n",
      "    (12): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): Conv1d(96, 96, kernel_size=(3,), stride=(1,))\n",
      "    (15): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (18): Conv1d(96, 192, kernel_size=(3,), stride=(1,))\n",
      "    (19): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU()\n",
      "    (21): Conv1d(192, 192, kernel_size=(3,), stride=(1,))\n",
      "    (22): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU()\n",
      "    (24): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (25): Conv1d(192, 384, kernel_size=(3,), stride=(1,))\n",
      "    (26): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU()\n",
      "    (28): Conv1d(384, 384, kernel_size=(3,), stride=(1,))\n",
      "    (29): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): ReLU()\n",
      "    (31): AdaptiveAvgPool1d(output_size=50)\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=19200, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "{'epochs': 20, 'classes': 4, 'kernels': [2205, 256], 'batch_size': 64, 'learning_rate': 0.005, 'dataset': 'MER_Taffc', 'architecture': '1D Convolution'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85565aa0168b421d91c55e984e8f008e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/research/lib/python3.7/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45135<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456ec053b48f41bc8b9ba2e35de5f607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/tharindu/Projects/research/notebooks/mer_taffc/wandb/run-20210324_071650-2zyk3xe4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/tharindu/Projects/research/notebooks/mer_taffc/wandb/run-20210324_071650-2zyk3xe4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">charmed-field-19</strong>: <a href=\"https://wandb.ai/maverickuor/baseline-audio-only/runs/2zyk3xe4\" target=\"_blank\">https://wandb.ai/maverickuor/baseline-audio-only/runs/2zyk3xe4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-78fbf03a599a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build, train and analyze the model with the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-8e5336bd5551>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# and use them to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# and test its final performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2601c56b85f5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, _val_loader, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mexample_ct\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mbatch_ct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2601c56b85f5>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(X, labels, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Backward pass ⬅\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Step with optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/research/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/research/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/research/lib/python3.7/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hook_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOW87r8l36Tr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AudioOnly-1D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research_kernel",
   "language": "python",
   "name": "research_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
